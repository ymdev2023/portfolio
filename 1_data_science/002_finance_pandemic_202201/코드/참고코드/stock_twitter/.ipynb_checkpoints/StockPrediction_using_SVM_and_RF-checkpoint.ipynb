{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:52:58.378946Z",
     "start_time": "2022-01-25T07:52:49.979592Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "jD8nVGIhrJLy",
    "outputId": "f954d7ed-c0ad-461a-8188-526804b47f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\ym\\anaconda3\\lib\\site-packages (0.1.69)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from yfinance) (1.3.4)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from yfinance) (0.0.10)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from yfinance) (1.20.3)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from yfinance) (2.26.0)\n",
      "Requirement already satisfied: lxml>=4.5.1 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from yfinance) (4.6.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from pandas>=0.24->yfinance) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
      "Collecting GetOldTweets3\n",
      "  Downloading GetOldTweets3-0.0.11-py3-none-any.whl (13 kB)\n",
      "Collecting pyquery>=1.2.10\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\ym\\anaconda3\\lib\\site-packages (from GetOldTweets3) (4.6.3)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
      "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.3\n",
      "Collecting treeinterpreter\n",
      "  Downloading treeinterpreter-0.2.3-py2.py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: treeinterpreter\n",
      "Successfully installed treeinterpreter-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install GetOldTweets3\n",
    "!pip install treeinterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:07.813768Z",
     "start_time": "2022-01-25T07:53:07.032598Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oX1x8Oj5rQ2_"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import GetOldTweets3 as got\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:08.428748Z",
     "start_time": "2022-01-25T07:53:08.420600Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Iz1xNzvKXwCs"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:13.677316Z",
     "start_time": "2022-01-25T07:53:11.282065Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "XxBZ351xze-e",
    "outputId": "0f469c92-222e-4e92-bf4b-a019be698949"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "sentiment_i_a = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:15.898105Z",
     "start_time": "2022-01-25T07:53:15.807904Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "aaSotX5Dy8DQ",
    "outputId": "96bab570-0bbd-40fb-abab-9dba570f79bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:18.051627Z",
     "start_time": "2022-01-25T07:53:18.035234Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z6UOzUuJsRkY"
   },
   "outputs": [],
   "source": [
    "#Get stock data for the given Stock Name\n",
    "def getStockDetails(stockname,start_time,end_time):\n",
    "  company = yf.Ticker(stockname)\n",
    "  company.info.get(\"longName\")\n",
    "  stockData = yf.download(stockname, start=start_time, end=end_time)\n",
    "  print(\"\\n Stock Data Obtained \")\n",
    "  print(stockData.head())\n",
    "  print(\"\\n\")\n",
    "  plt.title('Time series chart of Closing stocks for ' + company.info.get(\"longName\"))\n",
    "  plt.plot(stockData[\"Close\"])\n",
    "  plt.show()\n",
    "  print(\"\\n\")\n",
    "  stockData.to_csv('stockData_' + stockname + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:22.788504Z",
     "start_time": "2022-01-25T07:53:22.764429Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "P3EEVsKAW9We"
   },
   "outputs": [],
   "source": [
    "#Method for data cleaning\n",
    "class TweetCleaner:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.punc_table = str.maketrans(\"\", \"\", string.punctuation) # to remove punctuation from each word in tokenize\n",
    "\n",
    "    def compound_word_split(self, compound_word):\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)\n",
    "        return [m.group(0) for m in matches]\n",
    "\n",
    "    def remove_non_ascii_chars(self, text):\n",
    "        return ''.join([w if ord(w) < 128 else ' ' for w in text])\n",
    "\n",
    "    def remove_hyperlinks(self,text):\n",
    "        return ' '.join([w for w in text.split(' ')  if not 'http' in w])\n",
    "\n",
    "    def get_cleaned_text(self, text):\n",
    "        cleaned_tweet = text.replace('\\\"','').replace('\\'','').replace('-',' ')\n",
    "        cleaned_tweet =  self.remove_non_ascii_chars(cleaned_tweet)\n",
    "        if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_tweet):\n",
    "            cleaned_tweet = cleaned_tweet[cleaned_tweet.index(':')+2:]\n",
    "        cleaned_tweet = self.remove_hyperlinks(cleaned_tweet)\n",
    "        cleaned_tweet = cleaned_tweet.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL') # to avoid being removed while removing punctuations\n",
    "        tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_tweet)] # remove punctuations and tokenize\n",
    "        tokens = [nltk.WordNetLemmatizer().lemmatize(w) for w in tokens if not w.lower() in self.stop_words and len(w)>1] # remove stopwords and single length words\n",
    "        cleaned_tweet = ' '.join(tokens)\n",
    "        cleaned_tweet = cleaned_tweet.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')\n",
    "        cleaned_tweet = cleaned_tweet\n",
    "        return cleaned_tweet\n",
    "\n",
    "    def clean_tweets(self, tweets, is_bytes = False):   \n",
    "        test_tweet_list = []\n",
    "        for tweet in tweets:\n",
    "            if is_bytes:\n",
    "                test_tweet_list.append(self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\")))\n",
    "            else:\n",
    "                test_tweet_list.append(self.get_cleaned_text(tweet))\n",
    "        return test_tweet_list\n",
    "    \n",
    "    def clean_single_tweet(self, tweet, is_bytes = False):  \n",
    "        if is_bytes:\n",
    "             return self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\"))\n",
    "        return self.get_cleaned_text(tweet)\n",
    "    \n",
    "    def cleaned_file_creator(self, op_file_name, value1, value2):\n",
    "        csvFile = open(op_file_name, 'w+')\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        for tweet in range(len(value1)):\n",
    "            csvWriter.writerow([value1[tweet], value2[tweet]])\n",
    "        csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:27.797266Z",
     "start_time": "2022-01-25T07:53:27.778053Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8L64V6Mzrp4i"
   },
   "outputs": [],
   "source": [
    "#Method for fetching tweets\n",
    "def fetchTweets(stockname,start_time,end_time):\n",
    "  csvFile = open('tweets_' + stockname + '.csv', 'a',encoding=\"utf-8\")\n",
    "  csvWriter = csv.writer(csvFile, lineterminator= '\\n')\n",
    "  cleanObj = TweetCleaner()\n",
    "  \n",
    "  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(stockname).setSince(start_time).setUntil(end_time).setTopTweets(\"true\") \n",
    "  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "  try:\n",
    "    for tweet in tweets:\n",
    "      tweet_text = tweet.text.encode('utf-8')\n",
    "      tweet_text = cleanObj.get_cleaned_text(tweet_text.decode())\n",
    "      tweetDate = tweet.date.date()\n",
    "      csvWriter.writerow([tweetDate, tweet.text])\n",
    "  except BaseException as e:\n",
    "      print('failed on_status,',str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:31.815906Z",
     "start_time": "2022-01-25T07:53:31.799069Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ze2LxvtM3VnW"
   },
   "outputs": [],
   "source": [
    "#Method for processing tweets\n",
    "def processTweets(stockname):\n",
    "  columns=['Date','Tweets']\n",
    "  data = pd.DataFrame(columns)\n",
    "  df = pd.read_csv('tweets_' + stockname  + '.csv',encoding='utf-8', names=columns, header=None)\n",
    "  \n",
    "  indx=0\n",
    "  get_tweet=\"\"\n",
    "  #get tweets day wise\n",
    "  for i in range(0,len(df)-1):\n",
    "    get_date=df.Date.iloc[i]\n",
    "    next_date=df.Date.iloc[i+1]\n",
    "    if(str(get_date)==str(next_date)):\n",
    "      get_tweet = get_tweet + df.Tweets.iloc[i]+\" \"\n",
    "    if(str(get_date)!=str(next_date)):\n",
    "      data.at[indx,'Date'] = get_date\n",
    "      data.at[indx,'Tweets'] = get_tweet\n",
    "      indx=indx+1\n",
    "      get_tweet=\" \"\n",
    "\n",
    "  #get respective prices for each day using stockData\n",
    "  data['Prices']=\"\"\n",
    "  readStockData = pd.read_csv('stockData_' + stockname + '.csv')\n",
    "  readStockData.columns = [c.replace(' ', '_') for c in readStockData.columns]\n",
    "  for i in range (0,len(data)):\n",
    "      for j in range (0,len(readStockData)):\n",
    "          get_tweet_date = data.Date.iloc[i]\n",
    "          get_stock_date = readStockData.Date.iloc[j]\n",
    "          if(str(get_stock_date)==str(get_tweet_date)):\n",
    "            data.at[i,'Prices'] = int(readStockData.Adj_Close[j])\n",
    "            break\n",
    "\n",
    "  #drop rows that do not have Price values\n",
    "  data['Prices'].replace('', np.nan, inplace=True)\n",
    "  data.dropna(subset=['Prices'], inplace=True)\n",
    "  data.reset_index(drop=True, inplace=True)\n",
    "  data['Prices'] = data['Prices'].apply(np.int64)\n",
    "  data.drop(0, 1, inplace=True)\n",
    "  print(data.head())\n",
    "  data.to_csv('processedTweets_' + stockname  + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:36.153681Z",
     "start_time": "2022-01-25T07:53:36.132861Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TLsopWAOs7c5"
   },
   "outputs": [],
   "source": [
    "#Method for sentiment analysis for tweets\n",
    "def sentimentAnalysis(stockname):\n",
    "  data = pd.read_csv('processedTweets_' + stockname  + '.csv', encoding='utf-8')\n",
    "  data[\"Comp\"] = ''\n",
    "  data[\"Negative\"] = ''\n",
    "  data[\"Neutral\"] = ''\n",
    "  data[\"Positive\"] = ''\n",
    "  for indexx, row in data.T.iteritems():\n",
    "    try:\n",
    "      sentence_i = unicodedata.normalize('NFKD', data.loc[indexx, 'Tweets'])\n",
    "      sentence_sentiment = sentiment_i_a.polarity_scores(sentence_i)\n",
    "      data.at[indexx, 'Comp'] =  sentence_sentiment['compound']\n",
    "      data.at[indexx, 'Negative'] = sentence_sentiment['neg']\n",
    "      data.at[indexx, 'Neutral'] =  sentence_sentiment['neu']\n",
    "      data.at[indexx, 'Positive'] = sentence_sentiment['pos'] \n",
    "    except TypeError:\n",
    "      print('failed on_status,',str(e))\n",
    "\n",
    "  data.drop(['Unnamed: 0'], 1, inplace=True)\n",
    "  print(data.head())\n",
    "  data.to_csv('sentimentAnalysis_' + stockname  + '.csv')\n",
    "\n",
    "  posi=0\n",
    "  nega=0\n",
    "  neutral = 0\n",
    "  for i in range (0,len(data)):\n",
    "    get_val = data.Comp[i]\n",
    "    if(float(get_val)<(0)):\n",
    "        nega=nega+1\n",
    "    if(float(get_val>(0))):\n",
    "        posi=posi+1\n",
    "    if(float(get_val)==(0)):\n",
    "        neutral=neutral+1\n",
    "  \n",
    "  posper=(posi/(len(data)))*100\n",
    "  negper=(nega/(len(data)))*100\n",
    "  neutralper=(neutral/(len(data)))*100\n",
    "\n",
    "  arr=np.asarray([posper,negper,neutralper], dtype=int)\n",
    "  plt.figure()\n",
    "  plt.pie(arr,labels=['positive','negative', 'neutral'])\n",
    "  plt.plot()\n",
    "\n",
    "  print(\"% of positive tweets= \",posper)\n",
    "  print(\"% of negative tweets= \",negper)\n",
    "  print(\"% of neutral tweets= \",neutralper)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:37.947514Z",
     "start_time": "2022-01-25T07:53:37.937357Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5AenqyjW2yAa"
   },
   "outputs": [],
   "source": [
    "#Method for stock price prediction using random forest model\n",
    "def RandomForestModel(stockname):\n",
    "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
    "  train, test = train_test_split(df, shuffle=False, test_size=0.2)\n",
    "\n",
    "  sentiment_score_list_train = []\n",
    "  for date, row in train.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_train.append(sentiment_score)\n",
    "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
    "\n",
    "  sentiment_score_list_test = []\n",
    "  for date, row in test.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_test.append(sentiment_score)\n",
    "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
    "\n",
    "  y_train = pd.DataFrame(train['Prices'])\n",
    "  y_test = pd.DataFrame(test['Prices'])\n",
    "\n",
    "  rf = RandomForestRegressor()\n",
    "  rf.fit(numpy_df_train, y_train)\n",
    "  prediction, bias, contributions = ti.predict(rf, numpy_df_test)\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  plt.figure()\n",
    "  plt.plot(test['Prices'].iloc[:].values)\n",
    "  plt.plot(prediction.flatten())\n",
    "  plt.title('Random Forest predicted prices')\n",
    "  plt.ylabel('Stock Prices')\n",
    "  plt.xlabel('Days')\n",
    "  plt.legend(['actual', 'predicted'])\n",
    "  plt.show()\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(\"RMSE value for Random Forest Model : \")\n",
    "  rmse = sqrt(mean_squared_error(y_test, prediction.flatten()))\n",
    "  print(rmse)\n",
    "  print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T07:53:41.419420Z",
     "start_time": "2022-01-25T07:53:41.404439Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yDn4NcJDCldY"
   },
   "outputs": [],
   "source": [
    "#Method for stock price prediction using support vector regression model\n",
    "def SVRModel(stockname):\n",
    "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
    "  train, test = train_test_split(df, shuffle=False, test_size=0.2)\n",
    "\n",
    "  sentiment_score_list_train = []\n",
    "  for date, row in train.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_train.append(sentiment_score)\n",
    "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
    "\n",
    "  sentiment_score_list_test = []\n",
    "  for date, row in test.T.iteritems():\n",
    "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
    "    sentiment_score_list_test.append(sentiment_score)\n",
    "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
    "\n",
    "  y_train = pd.DataFrame(train['Prices'])\n",
    "  y_test = pd.DataFrame(test['Prices'])\n",
    "\n",
    "  svr_rbf = SVR(kernel='rbf', C=1e6, gamma=0.1)\n",
    "  svr_rbf.fit(numpy_df_train, y_train.values.flatten())\n",
    "  output_test_svm = svr_rbf.predict(numpy_df_test)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(test['Prices'].iloc[:].values)\n",
    "  plt.plot(output_test_svm)\n",
    "  plt.title('SVM predicted prices')\n",
    "  plt.ylabel('Stock Prices')\n",
    "  plt.xlabel('Days')\n",
    "  plt.legend(['actual', 'predicted'])\n",
    "  plt.show()\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(\"RMSE value for Support Vector Regression Model : \")\n",
    "  rmse = sqrt(mean_squared_error(y_test, output_test_svm))\n",
    "  print(rmse)\n",
    "  print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-25T07:54:40.115Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9eoV77Alrs5W",
    "outputId": "6b00dd30-8522-429d-f067-7db59481c893"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  name = input(\"Enter a valid STOCKNAME of the Corporation: \") #enter the name of the company\n",
    "  start_date = input(\"Enter the Start Date in the following format[YYYY-MM-DD]: \") #enter the start date to fetch the tweets\n",
    "  end_date = input(\"Enter the End Date in the following format[YYYY-MM-DD]: \" ) #enter the end date to fetch the tweets\n",
    "  \n",
    "  if(len(name) > 0):\n",
    "    STOCKNAME  = name\n",
    "  else:\n",
    "    STOCKNAME = \"AAPL\"\n",
    "  \n",
    "  if(len(start_date) > 0):\n",
    "    start_time = start_date\n",
    "  else:\n",
    "    start_time = \"2018-01-01\"\n",
    "  \n",
    "  if(len(end_date) > 0):\n",
    "    end_time = end_date\n",
    "  else:\n",
    "    end_time = \"2019-12-31\"\n",
    "\n",
    "\n",
    "  #Get Stock Details\n",
    "  print(\"------------------------------ Getting Stock details -----------------------------\")\n",
    "  stockData = getStockDetails(STOCKNAME,start_time,end_time)\n",
    "  print(\"Stock Details fetched! \\n\")\n",
    "\n",
    "  #Fetching tweets\n",
    "  print(\"------------------------------ Fetching Tweets -----------------------------\")\n",
    "  fetchTweets(STOCKNAME,start_time,end_time)\n",
    "  print(\"Tweets fetched! \\n\")\n",
    "\n",
    "  #Get tweets Per Day and get the stock closing values for each date\n",
    "  print(\"------------------------------ Processing Tweets -----------------------------\")\n",
    "  processTweets(STOCKNAME)\n",
    "  print(\"Processed Tweets ! \\n\")\n",
    "\n",
    "  #Perform Sentiment Analysis\n",
    "  print(\"------------------------------ Performing Sentiment Analysis -----------------------------\")\n",
    "  sentimentAnalysis(STOCKNAME)\n",
    "  print(\"Completed Sentiment Analysis on Tweets ! \\n\\n\")\n",
    "  #time.sleep(10);\n",
    "\n",
    "  #Training and Predicting using Random Forest Regression Model\n",
    "  print(\"--------  Training and Predicting using Random Forest Regression Model -------\")\n",
    "  RandomForestModel(STOCKNAME)\n",
    "  print(\"\\n \\n\")\n",
    "\n",
    "  #Training and Predicting using Support Vecor Regression Model\n",
    "  print(\"-------- Training and Predicting using Support Vector Regression Model ------------\")\n",
    "  SVRModel(STOCKNAME)\n",
    "  print(\"\\n \\n\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZfb5UFwl-3h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StockPrediction_using_SVM_and_RF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
